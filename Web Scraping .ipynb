{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping using Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sabarnna sen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeeksforGeeks | A computer science portal for geeks\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    " \n",
    "response = requests.get(\"https://www.geeksforgeeks.org/\") \n",
    "soup = BeautifulSoup(response.content, 'html.parser') \n",
    " \n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting HTML File After saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with open(\"test.html\") as fp: \\n    soup = BeautifulSoup(fp, \"html.parser\") \\nprint(soup.title.string)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open(\"test.html\") as fp: \n",
    "    soup = BeautifulSoup(fp, \"html.parser\") \n",
    "print(soup.title.string)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Interactions from Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interactionCount = soup.find(\\'meta\\', itemprop=\"interactionCount\") \\nprint(interactionCount[\\'content\\'])  \\n \\ndatePublished = soup.find(\\'meta\\', itemprop=\"datePublished\") \\nprint(datePublished[\\'content\\']) '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''interactionCount = soup.find('meta', itemprop=\"interactionCount\") \n",
    "print(interactionCount['content'])  \n",
    " \n",
    "datePublished = soup.find('meta', itemprop=\"datePublished\") \n",
    "print(datePublished['content']) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "internalLinks = [ \n",
    "a.get('href') for a in soup.find_all('a') \n",
    "if a.get('href') and a.get('href').startswith('/')] \n",
    "print(internalLinks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Social Sites from the main website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mailto:feedback@geeksforgeeks.org', 'https://www.facebook.com/geeksforgeeks.org/', 'https://twitter.com/geeksforgeeks']\n"
     ]
    }
   ],
   "source": [
    "links = [a.get('href') for a in soup.find_all('a')] \n",
    "to_extract = [\"facebook.com\", \"twitter.com\", \"mailto:\"] \n",
    "social_links = [] \n",
    "for link in links: \n",
    "    for social in to_extract: \n",
    "        if link and social in link: \n",
    "            social_links.append(link) \n",
    "print(social_links) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Emails using Regex function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feedback@geeksforgeeks.org', 'feedback@geeksforgeeks.org']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "emails = re.findall( \n",
    "  r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\", \n",
    "str(soup)) \n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Tables Automatically from the link <a href=\"https://en.wikipedia.org/wiki/List_of_best-selling_albums\">Best Selling Albums Wiki</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Artist', 'Album', 'Released', 'Genre', 'Total certified copies(from available markets)*', 'Claimed sales*', 'Ref(s)'], ['Michael Jackson', 'Thriller', '1982', 'Pop, post-disco, funk, rock', '47.3', '70', ''], ['AC/DC', 'Back in Black', '1980', 'Hard rock', '29.6', '50', ''], ['Meat Loaf', 'Bat Out of Hell', '1977', 'Hard rock, glam rock, progressive rock', '21.7', '50', ''], ['Pink Floyd', 'The Dark Side of the Moon', '1973', 'Progressive rock', '24.4', '45', ''], ['Whitney Houston / Various artists', 'The Bodyguard', '1992', 'R&B, soul, pop, soundtrack', '32.4', '45', ''], ['Eagles', 'Their Greatest Hits (1971â€“1975)', '1976', 'Country rock, soft rock, folk rock', '41.2', '42', ''], ['Bee Gees / Various artists', 'Saturday Night Fever', '1977', 'Disco', '21.6', '40', ''], ['Fleetwood Mac', 'Rumours', '1977', 'Soft rock', '27.9', '40', ''], ['Shania Twain', 'Come On Over', '1997', 'Country, pop', '29.6', '40', '']]\n"
     ]
    }
   ],
   "source": [
    "response =requests.get(\"https://en.wikipedia.org/wiki/List_of_best-selling_albums\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find(\"table\", class_=\"sortable\") \n",
    "output = [] \n",
    "for row in table.findAll(\"tr\"): \n",
    "    new_row = [] \n",
    "    for cell in row.findAll([\"td\", \"th\"]): \n",
    "        for sup in cell.findAll('sup'): \n",
    "            sup.extract() \n",
    "        for collapsible in cell.findAll( \n",
    "                class_=\"mw-collapsible-content\"): \n",
    "            collapsible.extract() \n",
    "        new_row.append(cell.get_text().strip()) \n",
    "    output.append(new_row) \n",
    "print(output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Can also be done using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Artist', 'Album', 'Released', 'Genre',\n",
      "       'Total certified copies(from available markets)*', 'Claimed sales*'],\n",
      "      dtype='object')\n",
      "Artist                                              object\n",
      "Album                                               object\n",
      "Released                                             int64\n",
      "Genre                                               object\n",
      "Total certified copies(from available markets)*    float64\n",
      "Claimed sales*                                       int64\n",
      "dtype: object\n",
      "422\n",
      "Artist                                                            Pink Floyd\n",
      "Album                                              The Dark Side of the Moon\n",
      "Released                                                                1973\n",
      "Genre                                                       Progressive rock\n",
      "Total certified copies(from available markets)*                         24.4\n",
      "Claimed sales*                                                            45\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    " \n",
    "table_df = pd.read_html(str(table))[0] \n",
    "table_df = table_df.drop('Ref(s)', 1) \n",
    "print(table_df.columns) # ['Artist', 'Album', 'Released' ... \n",
    "print(table_df.dtypes) # ... Released int64 ... \n",
    "print(table_df['Claimed sales*'].sum()) # 422 \n",
    "print(table_df.loc[3]) \n",
    "# Artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract from Metadata instead HTML from <a href=\"https://www.netflix.com/in/title/80189685\">Netflix The Witcher</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Henry Cavill', 'Anya Chalotra', 'Freya Allan']\n"
     ]
    }
   ],
   "source": [
    "response =requests.get(\"https://www.netflix.com/in/title/80189685\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "actors = soup.find(class_=\"item-starring\").find( \n",
    "class_=\"title-data-info-item-list\") \n",
    "print(actors.text.split(',')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Henry Cavill', 'Anya Chalotra', 'Freya Allan', 'Jodhi May', 'MyAnna Buring', 'Joey Batey', 'Eamon Farren', 'MimÃ® M Khayisa', 'BjÃ¶rn Hlynur Haraldsson', 'Adam Levy', 'Lars Mikkelsen', 'Royce Pierreson', 'Wilson Mbomio', 'Anna Shaffer']\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    " \n",
    "ldJson = soup.find(\"script\", type=\"application/ld+json\") \n",
    "parsedJson = json.loads(ldJson.contents[0]) \n",
    "print([actor['name'] for actor in parsedJson['actors']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Followers of Insta profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-b5ca0c09ba94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmetaDescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'description'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetaDescription\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "response =requests.get(\"https://www.instagram.com/sav_bowl_of_change/\") ## It is showng error since insta profile is blocked\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "metaDescription = soup.find(\"meta\", {'name': 'description'}) \n",
    "print(metaDescription['content']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Ecommerce Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n"
     ]
    }
   ],
   "source": [
    "response =requests.get(\"https://www.spigen.com/collections/tesla/products/tesla-model-3-ta100-sticker?variant=39270568230959\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "brand = soup.find('meta', itemprop=\"brand\") \n",
    "print(brand['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Rating and Review of a certain product from Ecommerce site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9\n",
      "57\n",
      "0.492kg\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "response =requests.get(\"https://nomz.com/collections/energy-bites/products/gift-box?variant=31459597090948\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "ldJson = soup.find(\"script\", type=\"application/ld+json\") \n",
    "parsedJson = json.loads(ldJson.contents[0]) \n",
    "print(parsedJson[\"aggregateRating\"][\"ratingValue\"]) # 4.9 \n",
    "print(parsedJson[\"aggregateRating\"][\"reviewCount\"]) # 57 \n",
    "print(parsedJson[\"weight\"]) # 0.492kg -> extra, not visible in UI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Details of the product can be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '2316', 'name': \"Freddie Freeman 'FREEMAN5' Pro Model\", 'category': 'New Arrivals, Wood Bats, Wood Bats/Pro Model', 'price': '169.99'}, {'id': '2258', 'name': 'Gleyber Torres GLEY25 Pro Model', 'category': 'New Arrivals, Wood Bats, Wood Bats/Pro Model', 'price': '169.99'}, {'id': '2256', 'name': 'Trea Turner TVT Pro Model', 'category': 'New Arrivals, Wood Bats, Wood Bats/Pro Model', 'price': '169.99'}, {'id': '1945', 'name': '6 Bat USA Professional Cut Bundle', 'category': 'Wood Bats, Wood Bats/Professional Cuts', 'price': '579.99'}, {'id': '1804', 'name': 'M-71 Pro Model', 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '159.99'}, {'id': '914', 'name': 'AP5 Custom Pro Model', 'category': 'Wood Bats, Wood Bats/Custom Pro', 'price': '179.99'}, {'id': '2312', 'name': \"Freddie Freeman 'FREEMAN5' Custom Pro Model\", 'category': 'Wood Bats, Wood Bats/Custom Pro', 'price': '189.99'}, {'id': '2311', 'name': 'Gleyber Torres GLEY25 Custom Pro Model', 'category': 'Wood Bats, Wood Bats/Custom Pro', 'price': '189.99'}, {'id': '2310', 'name': 'Trea Turner TVT Custom Pro Model', 'category': 'Wood Bats, Wood Bats/Custom Pro', 'price': '189.99'}, {'id': '1550', 'name': \"Youth Josh Donaldson 'Bringer of Rain' Pro Model\", 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '99.99'}, {'id': '1539', 'name': \"Josh Donaldson 'Bringer of Rain' Pro Model\", 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '169.99'}, {'id': '1538', 'name': 'AP5 Pro Model', 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '159.99'}, {'id': '1543', 'name': 'JB19 Pro Model', 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '159.99'}, {'id': '1939', 'name': 'AM22 Pro Model', 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '159.99'}, {'id': '917', 'name': 'CU26 Custom Pro Model', 'category': 'Wood Bats, Wood Bats/Custom Pro', 'price': '179.99'}, {'id': '1541', 'name': 'CU26 Pro Model', 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '159.99'}, {'id': '1547', 'name': 'Buster Posey POSEY28 Pro Model', 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '169.99'}, {'id': '1546', 'name': 'Francisco Lindor LINDY12 Pro Model', 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '169.99'}, {'id': '1548', 'name': 'Anthony Rizzo RIZZ44 Pro Model', 'category': 'Wood Bats, Wood Bats/Pro Model', 'price': '169.99'}, {'id': '933', 'name': 'Youth AP5 Custom Pro Model', 'category': 'Wood Bats, Wood Bats/Custom Pro', 'price': '109.99'}, {'id': '935', 'name': 'Youth CU26 Custom Pro Model', 'category': 'Wood Bats, Wood Bats/Custom Pro', 'price': '109.99'}]\n"
     ]
    }
   ],
   "source": [
    "response =requests.get(\"https://maruccisports.com/wood-bats/\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "products = [] \n",
    "cards = soup.find_all(class_=\"card\") \n",
    "for card in cards: \n",
    "    products.append({ \n",
    "        'id': card.get('data-entity-id'), \n",
    "        'name': card.get('data-name'), \n",
    "        'category': card.get('data-product-category'), \n",
    "        'price': card.get('data-product-price') \n",
    "    }) \n",
    "print(products) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
